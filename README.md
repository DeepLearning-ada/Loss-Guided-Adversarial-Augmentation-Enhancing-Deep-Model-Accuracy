The project introduces Loss-Guided Adversarial Data Augmentation (ADA), a lightweight robustness method that adds adversarial examples only for training samples with high loss, allowing the model to focus its defenses on the hardest inputs. This selective strategy avoids unnecessary computation while significantly improving robustness—achieving up to ~32.74% PGD-10 accuracy on CIFAR-10—outperforming standard and full-batch adversarial training approaches. The work also highlights epsilon overfitting, where robustness drops sharply if evaluation attacks use a larger perturbation budget than training. Finally, real-world testing through a backend–Postman pipeline confirmed that adversarial vulnerabilities persist outside the training environment, validating the practical relevance of the ADA approach.
